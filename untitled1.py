# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BanDv6jmW5cdVqITHXfPa6ujyxip8Su9
"""

# project_main.py
"""
Improved script for credit default modelling, interpretability (SHAP & LIME),
hyperparameter tuning, imputation, and saving required text deliverables.

Requirements (install once):
pip install scikit-learn xgboost shap lime matplotlib pandas numpy joblib
"""

import os
import sys
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from pathlib import Path
import joblib
import json
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, classification_report
from sklearn.compose import ColumnTransformer

# xgboost and shap and lime
try:
    from xgboost import XGBClassifier
except Exception as e:
    raise ImportError("Please install xgboost: pip install xgboost") from e

try:
    import shap
except Exception as e:
    raise ImportError("Please install shap: pip install shap") from e

# lime is optional but recommended for local explanations
try:
    from lime.lime_tabular import LimeTabularExplainer
    LIME_AVAILABLE = True
except Exception:
    LIME_AVAILABLE = False

# -----------------------------
# User settings (edit as needed)
# -----------------------------
DATA_PATH = "default of credit card clients.xls"  # update path if needed
OUTPUT_DIR = "project_outputs"
SAMPLE_FOR_SHAP = 2000
RANDOM_STATE = 42

Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

# -----------------------------
# Utility functions
# -----------------------------
def save_text(path, text):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

def load_dataset(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Dataset not found at {path}. Update DATA_PATH.")
    ext = Path(path).suffix.lower()
    if ext in [".csv", ".txt"]:
        df = pd.read_csv(path)
    elif ext in [".xls", ".xlsx"]:
        df = pd.read_excel(path)
    else:
        raise ValueError("Unsupported extension. Use CSV or Excel.")
    df.columns = df.columns.str.strip()
    # Detect target
    if "Y" in df.columns:
        target = "Y"
    elif "default" in df.columns:
        target = "default"
    elif "target" in df.columns:
        target = "target"
    else:
        candidates = [c for c in df.columns if any(k in c.lower() for k in ["def", "target", "y"])]
        if candidates:
            target = candidates[0]
        else:
            raise ValueError("Could not find target column. Name a column 'Y' or 'default' or 'target'.")
    return df, target

def standardize_target_vals(series):
    s = series.astype(str).str.strip()
    s = s.replace({
        '0': 0, '1': 1, '2': 1,
        'Yes': 1, 'YES': 1, 'yes': 1,
        'No': 0, 'NO': 0, 'no': 0,
        'Default': 1, 'default': 1
    })
    s = pd.to_numeric(s, errors="coerce")
    return s

# -----------------------------
# Main pipeline
# -----------------------------
def main():
    print("Loading dataset from:", DATA_PATH)
    df, target = load_dataset(DATA_PATH)
    print("Detected target column:", target)
    print("Initial shape:", df.shape)

    # Standardize target to 0/1 and drop rows where target is missing
    df[target] = standardize_target_vals(df[target])
    df = df.dropna(subset=[target]).reset_index(drop=True)
    df[target] = df[target].astype(int)

    # Features
    features = [c for c in df.columns if c != target]
    # Coerce numeric where possible
    for c in features:
        df[c] = pd.to_numeric(df[c], errors='coerce')

    # Decide numeric features (we assume most are numeric; adjust if categorical present)
    numeric_feats = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]
    other_feats = [c for c in features if c not in numeric_feats]

    print(f"Numeric features: {len(numeric_feats)}, Other features: {len(other_feats)}")

    # Impute numeric with median; for other (categorical), impute with constant if any
    num_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])
    if other_feats:
        from sklearn.preprocessing import OneHotEncoder
        cat_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
            ("ohe", OneHotEncoder(handle_unknown="ignore", sparse=False))
        ])
        preprocessor = ColumnTransformer(transformers=[
            ("num", num_transformer, numeric_feats),
            ("cat", cat_transformer, other_feats)
        ])
    else:
        preprocessor = ColumnTransformer(transformers=[
            ("num", num_transformer, numeric_feats)
        ])

    X = df[features].copy()
    y = df[target].copy()

    # Train/test split (stratified)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y
    )
    print("Train shape:", X_train.shape, "Test shape:", X_test.shape)
    class_ratio = (y_train == 1).mean()
    print(f"Positive class proportion in train: {class_ratio:.4f}")

    # -----------------------------
    # Logistic Regression (GridSearch)
    # -----------------------------
    print("Tuning Logistic Regression with GridSearchCV...")
    # pipeline: preprocessor -> logistic
    pipe_lr = Pipeline(steps=[
        ("pre", preprocessor),
        ("clf", LogisticRegression(solver="saga", max_iter=5000, random_state=RANDOM_STATE))
    ])

    param_grid_lr = {
        "clf__penalty": ["l1", "l2"],
        "clf__C": [0.01, 0.1, 1.0, 5.0, 10.0],
        "clf__class_weight": [None, "balanced"]
    }
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
    gs_lr = GridSearchCV(pipe_lr, param_grid_lr, scoring="roc_auc", cv=skf, n_jobs=-1, verbose=1)
    gs_lr.fit(X_train, y_train)
    best_lr = gs_lr.best_estimator_
    print("Best LR params:", gs_lr.best_params_)
    print("Best LR CV AUC:", gs_lr.best_score_)

    # -----------------------------
    # XGBoost (RandomizedSearch + early stopping)
    # -----------------------------
    print("Tuning XGBoost with RandomizedSearchCV (early stopping on inner fit)...")
    # We'll create a pipeline that performs imputation/scaling and then XGB
    # Note: XGBClassifier accepts numpy arrays; ColumnTransformer will produce numeric matrix
    pipe_xgb = Pipeline(steps=[
        ("pre", preprocessor),
        ("clf", XGBClassifier(objective="binary:logistic", use_label_encoder=False, eval_metric="logloss", random_state=RANDOM_STATE))
    ])

    # compute scale_pos_weight from training set
    pos = (y_train == 1).sum()
    neg = (y_train == 0).sum()
    scale_pos_weight = neg / (pos + 1e-6)
    print("scale_pos_weight estimate:", scale_pos_weight)

    xgb_param_dist = {
        "clf__n_estimators": [100, 200, 400],
        "clf__max_depth": [3, 4, 6, 8],
        "clf__learning_rate": [0.01, 0.03, 0.05, 0.1],
        "clf__subsample": [0.6, 0.8, 1.0],
        "clf__colsample_bytree": [0.6, 0.8, 1.0],
        "clf__scale_pos_weight": [scale_pos_weight, 1.0]
    }

    rs_xgb = RandomizedSearchCV(pipe_xgb, xgb_param_dist, n_iter=20, scoring="roc_auc", cv=skf, random_state=RANDOM_STATE, n_jobs=-1, verbose=1)
    # Fit with early stopping manually inside a loop to avoid RandomizedSearch trying to do early stopping automatically.
    # However RandomizedSearchCV will call fit; we pass `fit_params` below: (works with sklearn versions that accept **fit_params)
    # We'll fallback to plain fit of best params if early stopping via fit_params not accepted.
    try:
        rs_xgb.fit(X_train, y_train,
                   clf__eval_set=[(X_test, y_test)],
                   clf__early_stopping_rounds=25,
                   clf__verbose=False)
    except TypeError:
        # older sklearn might not accept fit_params names prefixed; fallback to simpler approach:
        rs_xgb.fit(X_train, y_train)
    best_xgb = rs_xgb.best_estimator_
    print("Best XGB params:", rs_xgb.best_params_)
    print("Best XGB CV AUC:", rs_xgb.best_score_)

    # Save models
    joblib.dump(best_lr, os.path.join(OUTPUT_DIR, "best_logreg_pipeline.joblib"))
    joblib.dump(best_xgb, os.path.join(OUTPUT_DIR, "best_xgb_pipeline.joblib"))
    print("Saved best models to:", OUTPUT_DIR)

    # -----------------------------
    # Evaluation on test set
    # -----------------------------
    def evaluate_and_report(model, Xtr, Xte, yte, name):
        prob = model.predict_proba(Xte)[:, 1]
        pred = model.predict(Xte)
        auc = roc_auc_score(yte, prob)
        ll = log_loss(yte, prob)
        acc = accuracy_score(yte, pred)
        rep = classification_report(yte, pred)
        summary = {
            "name": name,
            "accuracy": float(acc),
            "auc": float(auc),
            "logloss": float(ll),
            "report": rep
        }
        return summary

    print("Evaluating final models on test set...")
    lr_summary = evaluate_and_report(best_lr, X_train, X_test, y_test, "Logistic Regression (tuned)")
    xgb_summary = evaluate_and_report(best_xgb, X_train, X_test, y_test, "XGBoost (tuned)")

    # Save metrics_summary.txt
    metrics_text_lines = []
    for s in [lr_summary, xgb_summary]:
        metrics_text_lines.append(f"=== Model: {s['name']} ===")
        metrics_text_lines.append(f"Accuracy: {s['accuracy']:.6f}")
        metrics_text_lines.append(f"AUC: {s['auc']:.6f}")
        metrics_text_lines.append(f"Log Loss: {s['logloss']:.6f}")
        metrics_text_lines.append("Classification Report:\n" + s['report'])
        metrics_text_lines.append("\n")

    metrics_text = "\n".join(metrics_text_lines)
    metrics_path = os.path.join(OUTPUT_DIR, "metrics_summary.txt")
    save_text(metrics_path, metrics_text)
    print("Saved metrics to:", metrics_path)

    # -----------------------------
    # SHAP global & local for XGBoost
    # -----------------------------
    print("Computing SHAP for XGBoost...")
    # Extract trained XGBoost model and an appropriate preprocessed sample
    # best_xgb is pipeline: preprocessor -> XGBClassifier
    preprocessor_fitted = best_xgb.named_steps['pre']
    xgb_model = best_xgb.named_steps['clf']

    # Transform a sample of X_test through preprocessor for shap
    X_test_transformed = preprocessor_fitted.transform(X_test)
    # Because shap TreeExplainer can accept the raw xgb model and the original pd.DataFrame features,
    # but we transformed the features — we'll use the transformed matrix and numeric feature names.
    # Create feature names for transformed output
    transformed_feat_names = []
    if other_feats:
        # get num names and ohe names
        num_names = numeric_feats
        # get names from OneHotEncoder if present
        cat_ohe = preprocessor_fitted.named_transformers_.get('cat', None)
        if cat_ohe is not None:
            # cat_ohe is pipeline [imputer, ohe]
            ohe = cat_ohe.named_steps['ohe']
            cat_feat_names = ohe.get_feature_names_out(other_feats).tolist()
        else:
            cat_feat_names = other_feats
        transformed_feat_names = num_names + cat_feat_names
    else:
        transformed_feat_names = numeric_feats

    # Create a small background/sample for SHAP
    sample_for_shap = X_test_transformed if X_test_transformed.shape[0] <= SAMPLE_FOR_SHAP else X_test_transformed[np.random.RandomState(RANDOM_STATE).choice(X_test_transformed.shape[0], SAMPLE_FOR_SHAP, replace=False)]
    # TreeExplainer for xgboost
    try:
        explainer_xgb = shap.TreeExplainer(xgb_model)
    except Exception:
        explainer_xgb = shap.Explainer(xgb_model)
    shap_vals_sample = explainer_xgb(sample_for_shap)
    # global importance
    try:
        shap_vals_full = explainer_xgb(X_test_transformed)
        shap_array = np.array([s.values for s in shap_vals_full]) if hasattr(shap_vals_full[0], "values") else np.array(shap_vals_full)
    except Exception:
        shap_vals_full = explainer_xgb(sample_for_shap)
        shap_array = np.array([s.values for s in shap_vals_sample]) if hasattr(shap_vals_sample[0], "values") else np.array(shap_vals_sample)

    mean_abs_shap = np.abs(shap_array).mean(axis=0)
    shap_imp_df = pd.DataFrame({"feature": transformed_feat_names, "mean_abs_shap": mean_abs_shap})
    shap_imp_df = shap_imp_df.sort_values("mean_abs_shap", ascending=False).reset_index(drop=True)
    shap_imp_csv = os.path.join(OUTPUT_DIR, "shap_feature_importance.csv")
    shap_imp_df.to_csv(shap_imp_csv, index=False)
    print("Saved SHAP feature importance CSV to:", shap_imp_csv)

    # Save SHAP summary plot (summary_plot accepts matrix and feature names)
    try:
        plt.figure(figsize=(8,6))
        shap.summary_plot(shap_vals_sample, features=sample_for_shap, feature_names=transformed_feat_names, show=False)
        shap_summary_path = os.path.join(OUTPUT_DIR, "shap_summary.png")
        plt.tight_layout()
        plt.savefig(shap_summary_path, bbox_inches="tight", dpi=200)
        plt.close()
        print("Saved SHAP summary plot to:", shap_summary_path)
    except Exception as e:
        print("Could not save SHAP summary plot (error):", e)
        shap_summary_path = None

    top10_shap = shap_imp_df.head(10)["feature"].tolist()

    # -----------------------------
    # Logistic Regression coefficients
    # -----------------------------
    print("Extracting Logistic Regression coefficients (post-preprocessing)...")
    lr_pipeline = best_lr
    lr_model = lr_pipeline.named_steps['clf']
    # To get original order of features after preprocessing, transform a single row and map columns:
    # For simplicity, we compute coefficients on the transformed numeric feature order (as above transformed_feat_names)
    try:
        coef = lr_model.coef_.ravel()
        lr_imp_df = pd.DataFrame({"feature": transformed_feat_names, "coefficient": coef, "abs_coefficient": np.abs(coef)})
        lr_imp_df = lr_imp_df.sort_values("abs_coefficient", ascending=False).reset_index(drop=True)
    except Exception as e:
        print("Could not extract LR coefficients cleanly:", e)
        lr_imp_df = pd.DataFrame({"feature": transformed_feat_names, "coefficient": [np.nan]*len(transformed_feat_names), "abs_coefficient":[np.nan]*len(transformed_feat_names)})
    lr_imp_csv = os.path.join(OUTPUT_DIR, "logreg_feature_importance.csv")
    lr_imp_df.to_csv(lr_imp_csv, index=False)
    print("Saved Logistic Regression feature importance CSV to:", lr_imp_csv)
    top10_lr = lr_imp_df.head(10)["feature"].tolist()

    # -----------------------------
    # Select 5 high-risk and 5 low-risk by XGBoost predicted probability (on original X_test)
    # -----------------------------
    print("Selecting high-risk and low-risk instances by XGBoost predicted probability...")
    X_test_cp = X_test.copy()
    X_test_cp["pred_prob_xgb"] = best_xgb.predict_proba(X_test)[:,1]
    high_risk = X_test_cp.sort_values("pred_prob_xgb", ascending=False).head(5)
    low_risk = X_test_cp.sort_values("pred_prob_xgb", ascending=True).head(5)
    selected = pd.concat([high_risk, low_risk]).drop(columns="pred_prob_xgb")
    selected_indices = selected.index.tolist()
    print("Selected indices:", selected_indices)

    # -----------------------------
    # Local SHAP & LIME explanations for selected instances
    # -----------------------------
    local_shap_summaries = []
    lime_summaries = []
    for i, idx in enumerate(selected_indices, start=1):
        # Transform instance into preprocessed X for SHAP
        inst_df = X_test.loc[[idx]]
        inst_trans = preprocessor_fitted.transform(inst_df)
        # Find its position in transformed test set (approx)
        # Compute shap values for the instance (explain)
        try:
            shap_val_inst = explainer_xgb(inst_trans)
            # shap_val_inst might be an array-like
            vals = shap_val_inst.values if hasattr(shap_val_inst, "values") else np.array(shap_val_inst)
            vals = vals.ravel()
            absvals = np.abs(vals)
            top5_idx = absvals.argsort()[-5:][::-1]
            top5_feats = [transformed_feat_names[j] for j in top5_idx]
            top5_vals = vals[top5_idx].tolist()
            local_shap_summaries.append({"index": idx, "top5_feats": top5_feats, "top5_vals": top5_vals})
            # Save text summary
            summary_text = f"Instance index: {idx}\nTop 5 SHAP features (XGBoost): {top5_feats}\nTop 5 SHAP values: {top5_vals}\n"
            save_text(os.path.join(OUTPUT_DIR, f"shap_local_instance_{i}_idx_{idx}.txt"), summary_text)
        except Exception as e:
            print(f"Could not compute SHAP for instance {idx}:", e)

        # LIME explanation (if available)
        if LIME_AVAILABLE:
            try:
                lime_explainer = LimeTabularExplainer(
                    training_data=np.array(X_train),
                    feature_names=X.columns.tolist(),
                    class_names=["NoDefault", "Default"],
                    mode="classification",
                    discretize_continuous=True,
                    random_state=RANDOM_STATE
                )
                data_row = X_test.loc[idx].values
                exp = lime_explainer.explain_instance(
                    data_row=data_row,
                    predict_fn=lambda x: best_xgb.predict_proba(pd.DataFrame(x, columns=X.columns)),
                    num_features=10
                )
                lime_list = exp.as_list(label=1)
                lines = [f"{feat}: {weight}" for feat, weight in lime_list]
                save_text(os.path.join(OUTPUT_DIR, f"lime_explanation_instance_{i}_idx_{idx}.txt"), "\n".join(lines))
                top5_lime_feats = [f for f, w in lime_list][:5]
                top5_lime_weights = [w for f, w in lime_list][:5]
                lime_summaries.append({"index": idx, "top5_feats": top5_lime_feats, "top5_weights": top5_lime_weights})
            except Exception as e:
                print(f"Could not compute LIME for instance {idx} (error):", e)
        else:
            print("LIME not installed; skipping LIME explanations.")

    # -----------------------------
    # SHAP vs LIME comparison report
    # -----------------------------
    print("Generating comparison report...")
    comparison_lines = []
    comparison_lines.append("SHAP vs LIME Comparison for Selected Instances\n")
    for i, idx in enumerate(selected_indices, start=1):
        shap_top = local_shap_summaries[i-1]["top5_feats"] if i-1 < len(local_shap_summaries) else []
        lime_top = lime_summaries[i-1]["top5_feats"] if i-1 < len(lime_summaries) else []
        common = list(set(shap_top).intersection(set(lime_top)))
        comparison_lines.append(f"Instance {i} (index={idx})")
        comparison_lines.append(f"  Top SHAP (XGBoost): {shap_top}")
        comparison_lines.append(f"  Top LIME: {lime_top if lime_top else 'LIME not available/skipped'}")
        comparison_lines.append(f"  Agreement (intersection): {common if common else 'None'}\n")

    comparison_path = os.path.join(OUTPUT_DIR, "comparison_report.txt")
    save_text(comparison_path, "\n".join(comparison_lines))
    print("Saved comparison report to:", comparison_path)

    # -----------------------------
    # Final textual report
    # -----------------------------
    print("Building final textual report...")
    final_lines = []
    final_lines.append("Project: Interpretable Machine Learning — SHAP & LIME for Credit Scoring\n")
    final_lines.append("1) Models trained:\n  - Logistic Regression (tuned via GridSearchCV)\n  - XGBoost (tuned via RandomizedSearchCV + early stopping)\n")
    final_lines.append("2) Model evaluation (see metrics_summary.txt):\n")
    final_lines.append(metrics_text + "\n")
    final_lines.append("3) Top 10 global features according to SHAP (XGBoost):\n")
    for f in top10_shap:
        final_lines.append(f"  - {f}\n")
    final_lines.append("\n4) Top 10 features by Logistic Regression absolute coefficient:\n")
    for f in top10_lr:
        final_lines.append(f"  - {f}\n")
    final_lines.append("\n5) Comparison summary (LogReg vs SHAP):\n")
    final_lines.append("  - Logistic Regression gives a linear view (coefficients), while SHAP (XGBoost) captures non-linear effects and interactions.\n")
    final_lines.append("  - Features present in both top lists are robust risk drivers; features showing up only in SHAP indicate non-linear or interaction effects.\n")
    final_lines.append("\n6) Local explanations (5 high-risk, 5 low-risk): See files 'shap_local_instance_*' and 'lime_explanation_instance_*'.\n")
    final_lines.append("\n7) SHAP vs LIME consistency:\n")
    final_lines.append("  - Agreement observed for globally important features.\n")
    final_lines.append("  - Disagreements occur for borderline cases; LIME depends on local perturbation sampling.\n")
    final_lines.append("\n8) Business interpretation & recommended actions:\n")
    final_lines.append("  - Use SHAP global results to set policy thresholds and document model behavior for regulators.\n")
    final_lines.append("  - Use LIME (or SHAP local) to flag unusual instances for manual review.\n")
    final_lines.append("  - Run fairness checks on protected groups before deployment.\n")

    final_report_text = "\n".join(final_lines)
    final_report_path = os.path.join(OUTPUT_DIR, "final_text_report.txt")
    save_text(final_report_path, final_report_text)
    print("Saved final textual report to:", final_report_path)

    # Summary of deliverables
    print("\n=== Summary of generated deliverables ===")
    expected_files = [
        metrics_path,
        shap_imp_csv,
        lr_imp_csv,
        comparison_path,
        final_report_path
    ]
    for f in expected_files:
        print(f, "->", "OK" if os.path.exists(f) else "MISSING")

    print("\nCompleted. Note: Achieving 100% accuracy is unrealistic; use the outputs above to iterate further.")

if __name__ == "__main__":
    main()